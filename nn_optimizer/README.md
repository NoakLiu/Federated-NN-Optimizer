## Optimizer Features

- Activation Functions: Includes ReLU, Sigmoid, Tanh, Softmax, etc.
- Dropout: Provides dropout functionality for neural network regularization.
- Batch Normalization: Helps to speed up training and improve performance.
- Optimizers: Supports various optimization algorithms like SGD, Momentum, NAG, and Adagrad.