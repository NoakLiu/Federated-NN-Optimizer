{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integrated-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cathedral-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.47967057e+00,  9.06426369e-01,  1.25195572e+00, ...,\n",
       "         6.31729322e-01,  2.60908392e-01,  3.00000000e+00],\n",
       "       [ 9.94315784e+00, -9.58055259e+00,  5.06857801e+00, ...,\n",
       "        -3.18574521e-01, -2.67055346e-01,  8.00000000e+00],\n",
       "       [ 4.70429957e+00, -8.83720616e+00,  4.10928532e+00, ...,\n",
       "         2.65140024e-01,  1.67865616e-01,  8.00000000e+00],\n",
       "       ...,\n",
       "       [-1.52911933e+01,  2.50308666e+00, -2.27169405e-01, ...,\n",
       "        -6.99688324e-02, -6.08919845e-01,  5.00000000e+00],\n",
       "       [-5.85707877e+00,  2.04437491e+00,  3.65488937e+00, ...,\n",
       "         6.86344933e-03,  2.01131653e-02,  1.00000000e+00],\n",
       "       [-1.76542944e+00, -1.89117258e+00, -2.14885246e+00, ...,\n",
       "         1.87276097e-02,  2.38587672e-01,  7.00000000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.load(\"dataset/train_data.npy\", encoding='bytes')\n",
    "train_label = np.load(\"dataset/train_label.npy\", encoding='bytes')\n",
    "test_data = np.load(\"dataset/test_data.npy\", encoding='bytes')\n",
    "test_label = np.load(\"dataset/test_label.npy\", encoding='bytes')\n",
    "train_dataset = np.hstack([train_data, train_label])\n",
    "test_dataset = np.hstack([test_data, test_label])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-nevada",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "engaging-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check missing\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "from subprocess import TimeoutExpired\n",
    "def check_missing_data(df):\n",
    "    # check for any missing data in the df\n",
    "    check = list(df.isnull().sum())\n",
    "    miss = False\n",
    "    for i in check:\n",
    "        if i == 1:\n",
    "            miss = True\n",
    "            break\n",
    "    return miss\n",
    "print(check_missing_data(train_df))\n",
    "print(check_missing_data(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e9576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 129)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54775e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 129)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-nashville",
   "metadata": {},
   "source": [
    "## Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lucky-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard_scaler(object):\n",
    "    def __init__(self, mu = None, std = None):\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mu = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mu) / self.std\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "applied-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use train mu and sd to normalize test data\n",
    "scaler = Standard_scaler().fit(train_data)\n",
    "x_train_norm = scaler.transform(train_data)\n",
    "x_test_norm = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4512eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.523460647273202\n",
      "-9.657574846694638\n"
     ]
    }
   ],
   "source": [
    "print(x_train_norm.max())\n",
    "print(x_train_norm.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78cf52b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.651056000808705\n",
      "-8.263215822170624\n"
     ]
    }
   ],
   "source": [
    "print(x_test_norm.max())\n",
    "print(x_test_norm.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-river",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "severe-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __relu_deriv(self, a):\n",
    "       # 1 for x>=0 and 0 for x <0\n",
    "       # reference: https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy\n",
    "        return 1 * (a>=0)\n",
    "      \n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    \n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  a * (1 - a)\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1./(1+np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_deriv(self, a):\n",
    "        return np.exp(-a)/((1+np.exp(-a))**2)\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        # https://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/\n",
    "        # unstable and alwasy get NaN result due to floating point limitation\n",
    "        # a popular choice is to -max(x)\n",
    "        # https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "        mx = np.max(x,axis=1,keepdims = True)\n",
    "        x_exp = np.exp(x - mx)\n",
    "        x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "        res = x_exp / x_sum\n",
    "        \"\"\"\n",
    "        mx = np.max(x, axis = 1)\n",
    "        e = np.exp(x - mx)\n",
    "        res =  e / e.sum(axis = 1)\n",
    "        \"\"\"\n",
    "        return res\n",
    "        \n",
    "    def __softmax_deriv(self, y, y_pred):\n",
    "        return y_pred - y\n",
    "    \n",
    "    \n",
    "    def __init__(self,activation='relu'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv\n",
    "        elif activation == 'sigmoid':\n",
    "            self.f = self.__sigmoid\n",
    "            self.f_deriv = self.__sigmoid_deriv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: linear layer input\n",
    "        \"\"\"\n",
    "        x_out = self.f(x)\n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        delta = self.f_deriv(delta) * delta\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-nursery",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "square-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(object):\n",
    "    \"\"\"\n",
    "    Inverted dropout implementation of a MLP\n",
    "    reference: https://blog.csdn.net/huqinweI987/article/details/103229158\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob):\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, is_training = True):\n",
    "        if is_training:\n",
    "            self.mask = np.random.binomial(n=1, p = 1-self.dropout_prob, size = x.shape)\n",
    "            result = x * self.mask\n",
    "            return result/(1-self.dropout_prob)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        https://stats.stackexchange.com/questions/207481/dropout-backpropagation-implementation\n",
    "        \"\"\"\n",
    "        delta = delta * self.mask/(1-self.dropout_prob)\n",
    "        return delta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-compromise",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "southeast-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Normalization(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon = 1e-5, momentum = 0.9):\n",
    "        self.epsilon = epsilon \n",
    "        self.momentum = momentum\n",
    "        # for mini-batch training, need to keep a global record for mean and variance\n",
    "        self.global_mean = None\n",
    "        self.global_var = None\n",
    "        self.X = None\n",
    "        self.X_normalized = None\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        self.v_gamma = None\n",
    "        self.v_beta = None\n",
    "        \n",
    "    def forward(self, X, is_training = True):\n",
    "        \"\"\"\n",
    "        Batch normalization transfer for mini-batch data\n",
    "        reference: \n",
    "            1. Pytorch source: https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm1d\n",
    "            2. https://medium.com/analytics-vidhya/deep-learning-basics-batch-normalization-ae105f9f537e\n",
    "            3. https://stats.stackexchange.com/questions/219808/how-and-why-does-batch-normalization-use-moving-averages-to-track-the-accuracy-o\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        sample_mean = np.mean(X, axis = 0)\n",
    "        sample_var = np.var(X, axis = 0)\n",
    "        \n",
    "        if self.global_mean is None:\n",
    "            # initialize \n",
    "            self.global_mean = sample_mean\n",
    "            self.global_var =  sample_var\n",
    "            self.gamma = np.ones(D, dtype = X.dtype)\n",
    "            self.beta = np.zeros(D, dtype = X.dtype)\n",
    "            self.v_gamma = np.zeros(self.gamma.shape)\n",
    "            self.v_beta = np.zeros(self.beta.shape)\n",
    "\n",
    "        if is_training:\n",
    "            # running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "            self.global_mean = self.momentum * self.global_mean + (1 - self.momentum) * sample_mean\n",
    "            # running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "            self.globar_var = self.momentum * self.global_var + (1 - self.momentum) * sample_var\n",
    "            \n",
    "            X_hat = (X - self.global_mean)/np.sqrt(self.global_var + self.epsilon)\n",
    "            y = np.multiply(self.gamma, X_hat) + self.beta\n",
    "            \n",
    "        # for testing        \n",
    "        else:\n",
    "            X_hat = (X - self.global_mean)/np.sqrt(self.global_var + self.epsilon)\n",
    "            y = np.multiply(self.gamma, X_hat) + self.beta\n",
    "        \n",
    "        # save X\n",
    "        self.X = X\n",
    "        self.X_normalized = X_hat\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        reference: https://www.adityaagrawal.net/blog/deep_learning/bprop_batch_norm\n",
    "        http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "        \"\"\"\n",
    "        N,D = delta.shape\n",
    "        xmu = self.X-self.global_mean\n",
    "        inverse_sqrt_var = 1/np.sqrt(self.global_var + self.epsilon) # intermediate value \n",
    "        \n",
    "        self.dgamma = np.sum(delta*self.X_normalized, axis=0)\n",
    "        self.dbeta = np.sum(delta, axis=0)\n",
    "        \n",
    "        dxhat = delta * self.gamma\n",
    "        dvar = np.sum(dxhat*xmu - 0.5 * (self.global_var + self.epsilon)**(-1.5), axis=0)\n",
    "        dmu = -np.sum(dxhat*inverse_sqrt_var, axis = 0) - 1./N * dvar * np.sum(2*xmu, axis = 0)\n",
    "        \n",
    "        delta = dxhat*inverse_sqrt_var + dvar*2*xmu/N + dmu/N\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def update(self, optimizer, weight_decay):\n",
    "        if optimizer.__class__.__name__ == 'SGD':\n",
    "            self.beta = optimizer_object.update(self.beta, self.dbeta, weight_decay) #*  weight_decay\n",
    "            self.gamma = optimizer_object.update(self.gamma, self.dgamma, weight_decay) #*  weight_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-killing",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "awful-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr):\n",
    "        super(SGD, self).__init__(lr)\n",
    "    \n",
    "    def update(self, x, dfdx, weight_decay):\n",
    "        x -= self.lr * dfdx\n",
    "        self.lr = self.lr * weight_decay\n",
    "        return x\n",
    "    \n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"\n",
    "    reference: https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, momentum = 0.9, v = None):\n",
    "        super(Momentum, self).__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def update(self, v, x, dfdx, weight_decay):\n",
    "        \"\"\"\n",
    "        one single update for all weights\n",
    "        w: weight\n",
    "        grad: gradient\n",
    "        \"\"\"\n",
    "        v = v * self.momentum - self.lr * dfdx\n",
    "        x = x + v\n",
    "        self.lr = self.lr * weight_decay\n",
    "        return v, x \n",
    "\n",
    "class NAG(Optimizer):\n",
    "    def __init__(self, lr, momentum = 0.9, v = None):\n",
    "        super(NAG, self).__init__(lr)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update(self, v, x, dfdx, weight_decay):\n",
    "        v = v * self.momentum - self.lr * (x - self.momentum * v)\n",
    "        x = x - v\n",
    "        self.lr = self.lr * weight_decay\n",
    "        return v, x\n",
    "\n",
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, lr, epsilon = 1e-7):\n",
    "        super(Adagrad, self).__init__(lr)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def update(self, v, x, dfdx, weight_decay):\n",
    "        self.r += np.square(dfdx)\n",
    "        dx = dfdx * self.lr/(np.sqrt(self.r) + self.epsilon)\n",
    "        x = x - dx\n",
    "        self.lr = self.lr * weight_decay\n",
    "        return x\n",
    "\n",
    "# class Adam(Optimizer):\n",
    "#     def __init__(self, lr, rho1 = 0.9, rho2 = 0.999, epsilon = 1e-7):\n",
    "#         super(Adam, self).__init__(lr)\n",
    "#         self.rho1 = rho1\n",
    "#         self.rho2 = rho2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.m = None # first moment\n",
    "#         self.v = None # second moment\n",
    "#         self.time = 0\n",
    "        \n",
    "#     def update(self, x, dfdx):\n",
    "#         if self.m is None:\n",
    "#             self.m = np.zeros_like(x)\n",
    "#             self.v = np.zeros_like(x)\n",
    "            \n",
    "#         self.time += 1\n",
    "#         self.m = self.m * self.rho1 + (1 - self.rho1)* dfdx\n",
    "#         self.v = self.v * self.rho2 + (1 - self.rho2)* (dfdx**2)\n",
    "        \n",
    "#         m_hat = self.m/(1-self.rho1**self.time)\n",
    "#         v_hat = self.v/(1-self.rho2**self.time)\n",
    "        \n",
    "#         x = x - self.lr * m_hat/(np.sqrt(v_hat)+ self.epsilon)\n",
    "#         return x \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-domain",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disabled-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):    \n",
    "    def __init__(self, n_in, n_out, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_v = None\n",
    "        self.output = None \n",
    "        \n",
    "        # we randomly assign small values for the weights as the initiallization\n",
    "        \n",
    "        np.random.seed(2022)\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        \n",
    "        #self.W = np.ones((n_in, n_out))*4\n",
    "        #self.W = np.zeros((n_in, n_out))\n",
    "        #print(self.W)\n",
    "         \n",
    "        # we set the size of bias as the size of output dimension\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        # we set he size of weight gradation as the size of weight\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.v_W = np.zeros(self.W.shape)\n",
    "        self.v_b = np.zeros(self.b.shape)\n",
    "         \n",
    "    def forward(self, input_v):\n",
    "        '''\n",
    "        :type input_v: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''          \n",
    "        self.input_v= input_v\n",
    "        self.output = np.dot(input_v, self.W) + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta): \n",
    "        self.grad_W = np.atleast_2d(self.input_v).T.dot(np.atleast_2d(delta))\n",
    "\n",
    "        self.grad_b = np.mean(delta,axis = 0)\n",
    "        delta = np.dot(delta, self.W.T)\n",
    "        assert(self.grad_W.shape == self.W.shape)\n",
    "        assert(self.grad_b.shape == self.b.shape)\n",
    "        return delta\n",
    "    \n",
    "    def update(self, optimizer, weight_decay):\n",
    "        if optimizer.__class__.__name__ == 'SGD':\n",
    "            \n",
    "            self.W = optimizer.update(self.W, self.grad_W,weight_decay) #*  weight_decay\n",
    "            \n",
    "            self.b = optimizer.update(self.b, self.grad_b,weight_decay) #  weight_decay     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-inflation",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "continued-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/23289547/shuffle-two-list-at-once-with-same-order\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    X = X[indices]\n",
    "    y = y[indices] \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ultimate-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "\n",
    "    # for initiallization, the code will create all layers automatically based on the provided parameters.     \n",
    "    def __init__(self, n_in, n_out, layers, optimizer = None, activation = 'relu', activation_last_layer = 'softmax',\n",
    "                 dropout_ratio = 0, is_batch_normalization = False, momentum = 0.9):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "\n",
    "        # initialize layers\n",
    "        self.layers=[]\n",
    "        self.activation= activation\n",
    "        self.optimizer = optimizer\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        for i in range(len(layers)): \n",
    "            if i == 0:\n",
    "                # adding the first hidden layer\n",
    "                # we fix activation to be one kind to compare performance\n",
    "                self.layers.append(LinearLayer(n_in,layers[i]))\n",
    "            else:\n",
    "                # adding the rest of hidden layers\n",
    "                self.layers.append(LinearLayer(layers[i-1],layers[i]))\n",
    "                \n",
    "            # we assume the MLP structure is Linear -- Dropout -- Batch Normlization\n",
    "            # dropout_ratio == 0 means no dropout\n",
    "            if dropout_ratio != 0:\n",
    "                self.layers.append(Dropout(dropout_ratio))\n",
    "                \n",
    "            if is_batch_normalization:\n",
    "                self.layers.append(Batch_Normalization())\n",
    "            \n",
    "            if i!=0:\n",
    "                self.layers.append(Activation(activation))\n",
    "        \n",
    "        # adding the output layer\n",
    "        self.layers.append(LinearLayer(layers[-1], n_out))\n",
    "        self.layers.append(Activation(activation_last_layer))\n",
    "\n",
    "    # forward progress: pass the information through the layers and out the results of final output layer\n",
    "    def forward(self,input_v):\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(input_v)\n",
    "            input_v = output\n",
    "        return output\n",
    "   \n",
    "    # backward progress  \n",
    "    def backward(self,delta):\n",
    "        # print(delta)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    # define the objection/loss function, we use cross_entropy as the loss\n",
    "    def criterion_cross_entropy(self,y,y_hat):\n",
    "        \"\"\"\n",
    "        y_hat: batch_size * 1\n",
    "        y : batch_size * n_class\n",
    "        y_actual_onehot: one hot encoding of y_hat, (batch_size * n_class)\n",
    "        \"\"\"\n",
    "        # cross entropy\n",
    "        y_actual_onehot = np.eye(self.n_out)[y].reshape(-1, self.n_out)\n",
    "        \n",
    "        # restrict the range into [1e-12, 1-1e-12]\n",
    "        #y_predicted = np.clip(y_hat, 1e-12, 1-1e-12)\n",
    "        \n",
    "        #sumn = np.sum(np.exp(y_hat))\n",
    "        #y_predicted = y_hat/sumn\n",
    "        y_predicted = np.clip(y_hat, 1e-12, 1-1e-12)\n",
    "        \n",
    "        #print(y_actual_onehot.shape) (100,10)\n",
    "        #print(y_predicted.shape) (100,10)\n",
    "        #print(y_actual_onehot)\n",
    "        #print(y_predicted)\n",
    "        #print(y_actual_onehot)\n",
    "        #print(y_predicted)\n",
    "        \n",
    "        # sum by row\n",
    "        loss = - np.sum(np.multiply(y_actual_onehot, np.log(y_predicted)), axis = 1)\n",
    "        # add weight decay parameter\n",
    "#         weight_decay_loss = loss * weight_decay\n",
    "        \n",
    "        # derivative of cross entropy with softmax\n",
    "        # self.layers[-1] is the activation function\n",
    "        delta = self.layers[-1].f_deriv(y_actual_onehot, y_predicted)\n",
    "        # return loss and delta\n",
    "        return loss, delta\n",
    "\n",
    "    # update the network weights after backward.\n",
    "    def update(self,lr, weight_decay):\n",
    "        if self.optimizer is None or self.optimizer == 'SGD':\n",
    "            optimizer_object = SGD(lr)\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__ =='LinearLayer':\n",
    "                    layer.update(optimizer_object, weight_decay)\n",
    "                elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "                    layer.update(optimizer_object, weight_decay)\n",
    "#                 if layer.__class__.__name__ =='LinearLayer':\n",
    "#                     layer.W = optimizer_object.update(layer.W, layer.grad_W) *  weight_decay\n",
    "#                     print(layer.W)\n",
    "#                     layer.b = optimizer_object.update(layer.b, layer.grad_b) *  weight_decay\n",
    "#                     print(layer.b)\n",
    "#                 elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "#                     layer.beta = optimizer_object.update(layer.beta, layer.dbeta) *  weight_decay\n",
    "#                     layer.gamma = optimizer_object.update(layer.gamma, layer.dgamma) *  weight_decay\n",
    "        \n",
    "        elif self.optimizer == 'Momentum':\n",
    "            optimizer_object = Momentum(lr)\n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__ =='LinearLayer':\n",
    "                    layer.v_W, layer.W = optimizer_object.update(layer.v_W, layer.W, layer.grad_W, weight_decay)\n",
    "                    layer.v_b, layer.b = optimizer_object.update(layer.v_b, layer.b, layer.grad_b, weight_decay)\n",
    "\n",
    "                elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "                    layer.v_gamma, layer.gamma = optimizer_object.update(layer.v_gamma, layer.gamma, layer.dgamma, weight_decay)\n",
    "                    layer.v_beta, layer.beta = optimizer_object.update(layer.v_beta, layer.beta, layer.dbeta, weight_decay)\n",
    "        '''\n",
    "        elif self.optimizer == 'NAG':\n",
    "            optimizer_object = NAG(lr)\n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__ =='LinearLayer':\n",
    "                    layer.v_W, layer.W = optimizer_object.update(layer.v_W, layer.W, layer.grad_W)\n",
    "\n",
    "                    layer.v_b, layer.b = optimizer_object.update(layer.v_b, layer.b, layer.grad_b)\n",
    "\n",
    "                elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "                    layer.v_gamma, layer.gamma = optimizer_object.update(layer.v_gamma, layer.gamma, layer.dgamma)\n",
    "                    layer.v_beta, layer.beta = optimizer_object.update(layer.v_beta, layer.beta, layer.dbeta)\n",
    "        '''                    \n",
    "                \n",
    "#        elif self.optimizer == 'Adagrad':\n",
    "#            optimizer_object = Adagrad(lr)\n",
    "#        elif self.optimizer == 'Adam':\n",
    "#            optimizer_object = Adam(lr)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if layer.__class__.__name__ =='LinearLayer':\n",
    "                if self.optimizer is None or self.optimizer == 'SGD':\n",
    "                    optimizer_object = SGD(lr)\n",
    "                    layer.W = optimizer_object.update(layer.W, layer.grad_W,weight_decay)\n",
    "                    layer.b = optimizer_object.update(layer.b, layer.grad_b,weight_decay)\n",
    "\n",
    "                elif self.optimizer == 'Momentum':\n",
    "                    optimizer_object = Momentum(lr)\n",
    "                    layer.v_W, layer.W = optimizer_object.update(layer.v_W, layer.grad_W, layer.W,weight_decay)\n",
    "                    layer.v_b, layer.b = optimizer_object.update(layer.v_b, layer.grad_b, layer.b,weight_decay)\n",
    "                    \n",
    "    # define the training function\n",
    "    # it will return all losses within the whole training process.\n",
    "    def fit(self, X,y, is_shuffle = False, learning_rate=0.001, epochs=100, batch_size = 100, weight_decay = 0.95):\n",
    "        \"\"\"\n",
    "        Online learning with mini-batch training.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\" \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        # initialize loss array for epoch training \n",
    "        to_return = np.zeros(epochs)\n",
    "\n",
    "        n_batch = math.ceil(X.shape[0] / batch_size)\n",
    "        \n",
    "        batch_loss = np.zeros(n_batch)\n",
    "                \n",
    "        for k in range(epochs):\n",
    "            time_start = time.time()\n",
    "            y_pred = None\n",
    "            \n",
    "            if is_shuffle == True:\n",
    "                X, y = shuffle(X, y)\n",
    "            \n",
    "    \n",
    "            #initialize loss array for mini-batch training \n",
    "            batch_loss = np.zeros(n_batch)\n",
    "            \n",
    "            for j in range(n_batch):\n",
    "                \n",
    "                # decide samples for each mini-batch\n",
    "                if j != n_batch-1:\n",
    "                    X_batch = X[j*batch_size:(j+1)*batch_size]\n",
    "                    y_batch = y[j*batch_size:(j+1)*batch_size]\n",
    "                else:\n",
    "                    X_batch = X[j*batch_size:]\n",
    "                    y_batch = y[j*batch_size:]\n",
    "                    \n",
    "                # forward pass\n",
    "                y_batch_hat = self.forward(X_batch)\n",
    "                \n",
    "                sumn = np.sum(np.exp(y_batch_hat))\n",
    "                y_batch_hat = (np.exp(y_batch_hat)/sumn)*9\n",
    "\n",
    "                # calculate loss and backward pass\n",
    "                loss, delta = self.criterion_cross_entropy(y_batch, y_batch_hat)\n",
    "                self.backward(delta)\n",
    "                # update\n",
    "                self.update(learning_rate, weight_decay)\n",
    "                \n",
    "                # calculate loss and accuracy \n",
    "                batch_loss[j] = np.sum(loss)\n",
    "\n",
    "                #after argmax\n",
    "                y_batch_pred = np.argmax(y_batch_hat, axis = 1).reshape(-1,1)\n",
    "                print(y_batch_pred)\n",
    "#                 print(y_batch_hat)\n",
    "#                 print(y_batch_pred) \n",
    "                if y_pred is None:\n",
    "                    y_pred = y_batch_pred\n",
    "                else:\n",
    "                    y_pred = np.vstack((y_pred, y_batch_pred))\n",
    "                    \n",
    "            epoch_loss = np.mean(batch_loss)\n",
    "            \n",
    "            to_return[k] = epoch_loss\n",
    "            \n",
    "            # for every epoch, print time and loss\n",
    "            accuracy = np.sum(y_pred == y) / y.shape[0]\n",
    "            print(\"epoch {} loss {:.6f}, accuracy {:.6f}% \".format(k, epoch_loss, 100*accuracy))\n",
    "        return to_return\n",
    "    \n",
    "    # define the prediction function\n",
    "    # we can use predict function to predict the results of new data, by using the well-trained network.\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "545b8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2ba7e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-tattoo",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5c110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 468.849447, accuracy 15.918000% \n",
      "epoch 1 loss 469.395093, accuracy 16.716000% \n",
      "epoch 2 loss 469.621436, accuracy 16.784000% \n",
      "epoch 3 loss 469.731402, accuracy 16.802000% \n",
      "epoch 4 loss 469.791107, accuracy 16.840000% \n",
      "epoch 5 loss 469.827778, accuracy 16.842000% \n",
      "epoch 6 loss 469.852797, accuracy 16.836000% \n",
      "epoch 7 loss 469.871510, accuracy 16.844000% \n",
      "epoch 8 loss 469.886435, accuracy 16.856000% \n",
      "epoch 9 loss 469.898785, accuracy 16.868000% \n",
      "epoch 10 loss 469.909174, accuracy 16.872000% \n",
      "epoch 11 loss 469.917954, accuracy 16.878000% \n",
      "epoch 12 loss 469.925385, accuracy 16.876000% \n",
      "epoch 13 loss 469.931695, accuracy 16.878000% \n",
      "epoch 14 loss 469.937091, accuracy 16.880000% \n",
      "epoch 15 loss 469.941759, accuracy 16.884000% \n",
      "epoch 16 loss 469.945854, accuracy 16.890000% \n",
      "epoch 17 loss 469.949502, accuracy 16.892000% \n",
      "epoch 18 loss 469.952803, accuracy 16.896000% \n",
      "epoch 19 loss 469.955825, accuracy 16.892000% \n",
      "epoch 20 loss 469.958621, accuracy 16.886000% \n",
      "epoch 21 loss 469.961224, accuracy 16.884000% \n",
      "epoch 22 loss 469.963660, accuracy 16.878000% \n",
      "epoch 23 loss 469.965945, accuracy 16.878000% \n",
      "epoch 24 loss 469.968089, accuracy 16.882000% \n",
      "epoch 25 loss 469.970104, accuracy 16.882000% \n",
      "epoch 26 loss 469.971996, accuracy 16.882000% \n",
      "epoch 27 loss 469.973775, accuracy 16.886000% \n",
      "epoch 28 loss 469.975449, accuracy 16.886000% \n",
      "epoch 29 loss 469.977026, accuracy 16.880000% \n",
      "epoch 30 loss 469.978513, accuracy 16.880000% \n",
      "epoch 31 loss 469.979917, accuracy 16.882000% \n",
      "epoch 32 loss 469.981244, accuracy 16.880000% \n",
      "epoch 33 loss 469.982500, accuracy 16.880000% \n",
      "epoch 34 loss 469.983688, accuracy 16.878000% \n",
      "epoch 35 loss 469.984814, accuracy 16.878000% \n",
      "epoch 36 loss 469.985882, accuracy 16.878000% \n",
      "epoch 37 loss 469.986895, accuracy 16.876000% \n",
      "epoch 38 loss 469.987855, accuracy 16.876000% \n",
      "epoch 39 loss 469.988767, accuracy 16.878000% \n",
      "epoch 40 loss 469.989633, accuracy 16.880000% \n",
      "epoch 41 loss 469.990455, accuracy 16.880000% \n",
      "epoch 42 loss 469.991236, accuracy 16.878000% \n",
      "epoch 43 loss 469.991978, accuracy 16.876000% \n",
      "epoch 44 loss 469.992683, accuracy 16.878000% \n",
      "epoch 45 loss 469.993354, accuracy 16.876000% \n",
      "epoch 46 loss 469.993993, accuracy 16.876000% \n",
      "epoch 47 loss 469.994601, accuracy 16.878000% \n",
      "epoch 48 loss 469.995180, accuracy 16.876000% \n",
      "epoch 49 loss 469.995732, accuracy 16.876000% \n",
      "epoch 50 loss 469.996259, accuracy 16.878000% \n",
      "epoch 51 loss 469.996762, accuracy 16.876000% \n",
      "epoch 52 loss 469.997242, accuracy 16.876000% \n",
      "epoch 53 loss 469.997702, accuracy 16.878000% \n",
      "epoch 54 loss 469.998143, accuracy 16.878000% \n",
      "epoch 55 loss 469.998565, accuracy 16.880000% \n",
      "epoch 56 loss 469.998970, accuracy 16.880000% \n",
      "epoch 57 loss 469.999359, accuracy 16.880000% \n",
      "epoch 58 loss 469.999733, accuracy 16.880000% \n",
      "epoch 59 loss 470.000093, accuracy 16.880000% \n",
      "epoch 60 loss 470.000440, accuracy 16.880000% \n",
      "epoch 61 loss 470.000774, accuracy 16.880000% \n",
      "epoch 62 loss 470.001097, accuracy 16.882000% \n",
      "epoch 63 loss 470.001409, accuracy 16.882000% \n",
      "epoch 64 loss 470.001711, accuracy 16.880000% \n",
      "epoch 65 loss 470.002003, accuracy 16.882000% \n",
      "epoch 66 loss 470.002286, accuracy 16.884000% \n",
      "epoch 67 loss 470.002561, accuracy 16.884000% \n",
      "epoch 68 loss 470.002827, accuracy 16.884000% \n",
      "epoch 69 loss 470.003086, accuracy 16.884000% \n",
      "epoch 70 loss 470.003338, accuracy 16.884000% \n",
      "epoch 71 loss 470.003583, accuracy 16.886000% \n",
      "epoch 72 loss 470.003822, accuracy 16.886000% \n",
      "epoch 73 loss 470.004055, accuracy 16.884000% \n",
      "epoch 74 loss 470.004283, accuracy 16.884000% \n",
      "epoch 75 loss 470.004505, accuracy 16.884000% \n",
      "epoch 76 loss 470.004722, accuracy 16.886000% \n",
      "epoch 77 loss 470.004934, accuracy 16.888000% \n",
      "epoch 78 loss 470.005142, accuracy 16.888000% \n",
      "epoch 79 loss 470.005346, accuracy 16.888000% \n",
      "epoch 80 loss 470.005545, accuracy 16.888000% \n",
      "epoch 81 loss 470.005741, accuracy 16.888000% \n",
      "epoch 82 loss 470.005933, accuracy 16.888000% \n",
      "epoch 83 loss 470.006122, accuracy 16.888000% \n",
      "epoch 84 loss 470.006307, accuracy 16.888000% \n",
      "epoch 85 loss 470.006490, accuracy 16.888000% \n",
      "epoch 86 loss 470.006669, accuracy 16.890000% \n",
      "epoch 87 loss 470.006845, accuracy 16.890000% \n",
      "epoch 88 loss 470.007019, accuracy 16.888000% \n",
      "epoch 89 loss 470.007190, accuracy 16.888000% \n",
      "epoch 90 loss 470.007358, accuracy 16.888000% \n",
      "epoch 91 loss 470.007524, accuracy 16.888000% \n",
      "epoch 92 loss 470.007688, accuracy 16.888000% \n",
      "epoch 93 loss 470.007849, accuracy 16.888000% \n",
      "epoch 94 loss 470.008008, accuracy 16.888000% \n",
      "epoch 95 loss 470.008165, accuracy 16.886000% \n",
      "epoch 96 loss 470.008320, accuracy 16.886000% \n",
      "epoch 97 loss 470.008472, accuracy 16.888000% \n",
      "epoch 98 loss 470.008623, accuracy 16.888000% \n",
      "epoch 99 loss 470.008771, accuracy 16.888000% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([468.84944653, 469.39509314, 469.62143593, 469.73140189,\n",
       "       469.79110723, 469.82777828, 469.85279695, 469.87150957,\n",
       "       469.88643508, 469.89878538, 469.90917394, 469.9179537 ,\n",
       "       469.92538468, 469.93169477, 469.93709139, 469.94175876,\n",
       "       469.94585351, 469.94950238, 469.95280261, 469.95582533,\n",
       "       469.95862091, 469.96122448, 469.96366019, 469.96594458,\n",
       "       469.96808931, 469.97010368, 469.97199624, 469.97377539,\n",
       "       469.97544942, 469.97702628, 469.97851334, 469.97991733,\n",
       "       469.98124431, 469.98249969, 469.98368831, 469.98481446,\n",
       "       469.98588205, 469.98689457, 469.98785524, 469.98876701,\n",
       "       469.98963264, 469.99045472, 469.99123566, 469.99197778,\n",
       "       469.99268328, 469.99335426, 469.99399272, 469.99460059,\n",
       "       469.9951797 , 469.99573181, 469.9962586 , 469.99676163,\n",
       "       469.99724242, 469.99770238, 469.99814283, 469.99856501,\n",
       "       469.99897009, 469.99935916, 469.99973322, 470.00009322,\n",
       "       470.00044004, 470.00077449, 470.00109734, 470.00140929,\n",
       "       470.00171101, 470.00200311, 470.00228617, 470.00256073,\n",
       "       470.00282728, 470.0030863 , 470.00333823, 470.00358346,\n",
       "       470.0038224 , 470.00405538, 470.00428276, 470.00450484,\n",
       "       470.00472191, 470.00493425, 470.0051421 , 470.00534572,\n",
       "       470.00554531, 470.00574108, 470.00593322, 470.00612192,\n",
       "       470.00630732, 470.00648958, 470.00666885, 470.00684524,\n",
       "       470.00701887, 470.00718985, 470.00735828, 470.00752425,\n",
       "       470.00768783, 470.0078491 , 470.00800813, 470.00816496,\n",
       "       470.00831967, 470.00847229, 470.00862287, 470.00877144])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input_training = x_train_norm.shape[1]\n",
    "n_class = len(np.unique(test_label.reshape(1, -1)))\n",
    "layers = [50, 70]\n",
    "optimizer = 'SGD'\n",
    "nn = MLP(n_input_training, n_class, layers, optimizer, activation = 'relu', activation_last_layer = 'softmax',\n",
    "                 dropout_ratio = 0, is_batch_normalization = False)\n",
    "nn.fit(x_train_norm, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24123a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
