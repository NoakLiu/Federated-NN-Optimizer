{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "integrated-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cathedral-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.47967057e+00,  9.06426369e-01,  1.25195572e+00, ...,\n",
       "         6.31729322e-01,  2.60908392e-01,  3.00000000e+00],\n",
       "       [ 9.94315784e+00, -9.58055259e+00,  5.06857801e+00, ...,\n",
       "        -3.18574521e-01, -2.67055346e-01,  8.00000000e+00],\n",
       "       [ 4.70429957e+00, -8.83720616e+00,  4.10928532e+00, ...,\n",
       "         2.65140024e-01,  1.67865616e-01,  8.00000000e+00],\n",
       "       ...,\n",
       "       [-1.52911933e+01,  2.50308666e+00, -2.27169405e-01, ...,\n",
       "        -6.99688324e-02, -6.08919845e-01,  5.00000000e+00],\n",
       "       [-5.85707877e+00,  2.04437491e+00,  3.65488937e+00, ...,\n",
       "         6.86344933e-03,  2.01131653e-02,  1.00000000e+00],\n",
       "       [-1.76542944e+00, -1.89117258e+00, -2.14885246e+00, ...,\n",
       "         1.87276097e-02,  2.38587672e-01,  7.00000000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.load(\"dataset/train_data.npy\", encoding='bytes')\n",
    "train_label = np.load(\"dataset/train_label.npy\", encoding='bytes')\n",
    "test_data = np.load(\"dataset/test_data.npy\", encoding='bytes')\n",
    "test_label = np.load(\"dataset/test_label.npy\", encoding='bytes')\n",
    "train_dataset = np.hstack([train_data, train_label])\n",
    "test_dataset = np.hstack([test_data, test_label])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-nevada",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaging-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check missing\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "from subprocess import TimeoutExpired\n",
    "def check_missing_data(df):\n",
    "    # check for any missing data in the df\n",
    "    check = list(df.isnull().sum())\n",
    "    miss = False\n",
    "    for i in check:\n",
    "        if i == 1:\n",
    "            miss = True\n",
    "            break\n",
    "    return miss\n",
    "print(check_missing_data(train_df))\n",
    "print(check_missing_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-nashville",
   "metadata": {},
   "source": [
    "## Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "lucky-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard_scaler(object):\n",
    "    def __init__(self, mu = None, std = None):\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mu = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mu) / self.std\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "applied-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use train mu and sd to normalize test data\n",
    "scaler = Standard_scaler().fit(train_data)\n",
    "x_train_norm = scaler.transform(train_data)\n",
    "x_test_norm = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-river",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "severe-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __relu_deriv(self, a):\n",
    "       # 1 for x>=0 and 0 for x <0\n",
    "       # reference: https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy\n",
    "        return 1 * (a>=0)\n",
    "      \n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    \n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  a * (1 - a)\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        # https://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/\n",
    "        # unstable and alwasy get NaN result due to floating point limitation\n",
    "        # a popular choice is to -max(x)\n",
    "        # https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "#         mx = np.max(x,axis=1,keepdims = True)\n",
    "#         x_exp = np.exp(x - mx)\n",
    "#         x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "#         res = x_exp / x_sum\n",
    "        mx = np.max(x, axis = 1)\n",
    "        e = np.exp(x - mx)\n",
    "        res =  e / e.sum(axis = 1)\n",
    "        return res\n",
    "       \n",
    "        \n",
    "    def __softmax_deriv(self, y, y_pred):\n",
    "        return y_pred - y\n",
    "    \n",
    "    def __init__(self,activation='relu'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: linear layer input\n",
    "        \"\"\"\n",
    "        x_out = self.f(x)\n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        delta = self.f_deriv(delta) * delta\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-nursery",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "square-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(object):\n",
    "    \"\"\"\n",
    "    Inverted dropout implementation of a MLP\n",
    "    reference: https://blog.csdn.net/huqinweI987/article/details/103229158\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob):\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, is_training = True):\n",
    "        if is_training:\n",
    "            self.mask = np.random.binomial(n=1, p = 1-self.dropout_prob, size = x.shape)\n",
    "            result = x * self.mask\n",
    "            return result/(1-self.dropout_prob)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        https://stats.stackexchange.com/questions/207481/dropout-backpropagation-implementation\n",
    "        \"\"\"\n",
    "        delta = delta * self.mask/(1-self.dropout_prob)\n",
    "        return delta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-compromise",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "southeast-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Normalization(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon = 1e-5, momentum = 0.9):\n",
    "        self.epsilon = epsilon \n",
    "        self.momentum = momentum\n",
    "        # for mini-batch training, need to keep a global record for mean and variance\n",
    "        self.global_mean = None\n",
    "        self.global_var = None\n",
    "        self.X = None\n",
    "        self.X_normalized = None\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        self.v_gamma = None\n",
    "        self.v_beta = None\n",
    "        \n",
    "    def forward(self, X, is_training = True):\n",
    "        \"\"\"\n",
    "        Batch normalization transfer for mini-batch data\n",
    "        reference: \n",
    "            1. Pytorch source: https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm1d\n",
    "            2. https://medium.com/analytics-vidhya/deep-learning-basics-batch-normalization-ae105f9f537e\n",
    "            3. https://stats.stackexchange.com/questions/219808/how-and-why-does-batch-normalization-use-moving-averages-to-track-the-accuracy-o\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        sample_mean = np.mean(X, axis = 0)\n",
    "        sample_var = np.var(X, axis = 0)\n",
    "        \n",
    "        if self.global_mean is None:\n",
    "            # initialize \n",
    "            self.global_mean = sample_mean\n",
    "            self.global_var =  sample_var\n",
    "            self.gamma = np.ones(D, dtype = X.dtype)\n",
    "            self.beta = np.zeros(D, dtype = X.dtype)\n",
    "            self.v_gamma = np.zeros(self.gamma.shape)\n",
    "            self.v_beta = np.zeros(self.beta.shape)\n",
    "\n",
    "        if is_training:\n",
    "            # running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "            self.global_mean = self.momentum * self.global_mean + (1 - self.momentum) * sample_mean\n",
    "            # running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "            self.globar_var = self.momentum * self.global_var + (1 - self.momentum) * sample_var\n",
    "            \n",
    "            X_hat = (X - self.global_mean)/np.sqrt(self.global_var + self.epsilon)\n",
    "            y = np.multiply(self.gamma, X_hat) + self.beta\n",
    "            \n",
    "        # for testing        \n",
    "        else:\n",
    "            X_hat = (X - self.global_mean)/np.sqrt(self.global_var + self.epsilon)\n",
    "            y = np.multiply(self.gamma, X_hat) + self.beta\n",
    "        \n",
    "        # save X\n",
    "        self.X = X\n",
    "        self.X_normalized = X_hat\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        reference: https://www.adityaagrawal.net/blog/deep_learning/bprop_batch_norm\n",
    "        http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "        \"\"\"\n",
    "        N,D = delta.shape\n",
    "        xmu = self.X-self.global_mean\n",
    "        inverse_sqrt_var = 1/np.sqrt(self.global_var + self.epsilon) # intermediate value \n",
    "        \n",
    "        self.dgamma = np.sum(delta*self.X_normalized, axis=0)\n",
    "        self.dbeta = np.sum(delta, axis=0)\n",
    "        \n",
    "        dxhat = delta * self.gamma\n",
    "        dvar = np.sum(dxhat*xmu - 0.5 * (self.global_var + self.epsilon)**(-1.5), axis=0)\n",
    "        dmu = -np.sum(dxhat*inverse_sqrt_var, axis = 0) - 1./N * dvar * np.sum(2*xmu, axis = 0)\n",
    "        \n",
    "        delta = dxhat*inverse_sqrt_var + dvar*2*xmu/N + dmu/N\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def update(self, optimizer, weight_decay):\n",
    "        if optimizer.__class__.__name__ == 'SGD':\n",
    "            self.beta = optimizer_object.update(self.beta, self.dbeta) *  weight_decay\n",
    "            self.gamma = optimizer_object.update(self.gamma, self.dgamma) *  weight_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-killing",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "awful-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr):\n",
    "        super(SGD, self).__init__(lr)\n",
    "    \n",
    "    def update(self, x, dfdx):\n",
    "        x -= self.lr * dfdx\n",
    "        return x\n",
    "    \n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"\n",
    "    reference: https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, momentum = 0.9, v = None):\n",
    "        super(Momentum, self).__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def update(self, v, x, dfdx):\n",
    "        \"\"\"\n",
    "        one single update for all weights\n",
    "        w: weight\n",
    "        grad: gradient\n",
    "        \"\"\"\n",
    "        v = v * self.momentum - self.lr * dfdx\n",
    "        x = x + v\n",
    "        return v, x \n",
    "\n",
    "class NAG(Optimizer):\n",
    "    def __init__(self, lr, momentum = 0.9, v = None):\n",
    "        super(NAG, self).__init__(lr)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update(self, v, x, dfdx):\n",
    "        v = v * self.momentum - self.lr * (x - self.momentum * v)\n",
    "        x = x - v\n",
    "        return v, x\n",
    "\n",
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, lr, epsilon = 1e-7):\n",
    "        super(Adagrad, self).__init__(lr)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def update(self, v, x, dfdx):\n",
    "        self.r += np.square(dfdx)\n",
    "        dx = dfdx * self.lr/(np.sqrt(self.r) + self.epsilon)\n",
    "        x = x - dx\n",
    "        return x\n",
    "\n",
    "# class Adam(Optimizer):\n",
    "#     def __init__(self, lr, rho1 = 0.9, rho2 = 0.999, epsilon = 1e-7):\n",
    "#         super(Adam, self).__init__(lr)\n",
    "#         self.rho1 = rho1\n",
    "#         self.rho2 = rho2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.m = None # first moment\n",
    "#         self.v = None # second moment\n",
    "#         self.time = 0\n",
    "        \n",
    "#     def update(self, x, dfdx):\n",
    "#         if self.m is None:\n",
    "#             self.m = np.zeros_like(x)\n",
    "#             self.v = np.zeros_like(x)\n",
    "            \n",
    "#         self.time += 1\n",
    "#         self.m = self.m * self.rho1 + (1 - self.rho1)* dfdx\n",
    "#         self.v = self.v * self.rho2 + (1 - self.rho2)* (dfdx**2)\n",
    "        \n",
    "#         m_hat = self.m/(1-self.rho1**self.time)\n",
    "#         v_hat = self.v/(1-self.rho2**self.time)\n",
    "        \n",
    "#         x = x - self.lr * m_hat/(np.sqrt(v_hat)+ self.epsilon)\n",
    "#         return x \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-domain",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "disabled-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):    \n",
    "    def __init__(self, n_in, n_out, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_v = None\n",
    "        self.output = None \n",
    "        \n",
    "        # we randomly assign small values for the weights as the initiallization\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "         \n",
    "        # we set the size of bias as the size of output dimension\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        # we set he size of weight gradation as the size of weight\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.v_W = np.zeros(self.W.shape)\n",
    "        self.v_b = np.zeros(self.b.shape)\n",
    "         \n",
    "    def forward(self, input_v):\n",
    "        '''\n",
    "        :type input_v: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''          \n",
    "        self.input_v= input_v\n",
    "        self.output = np.dot(input_v, self.W) + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta): \n",
    "        self.grad_W = np.atleast_2d(self.input_v).T.dot(np.atleast_2d(delta))\n",
    "\n",
    "        self.grad_b = np.mean(delta,axis = 0)\n",
    "        delta = np.dot(delta, self.W.T)\n",
    "        assert(self.grad_W.shape == self.W.shape)\n",
    "        assert(self.grad_b.shape == self.b.shape)\n",
    "        return delta\n",
    "    \n",
    "    def update(self, optimizer, weight_decay):\n",
    "        if optimizer.__class__.__name__ == 'SGD':\n",
    "            \n",
    "            self.W = optimizer.update(self.W, self.grad_W) *  weight_decay\n",
    "            \n",
    "            self.b = optimizer.update(self.b, self.grad_b) *  weight_decay\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-inflation",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "continued-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/23289547/shuffle-two-list-at-once-with-same-order\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    X = X[indices]\n",
    "    y = y[indices] \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ultimate-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "\n",
    "    # for initiallization, the code will create all layers automatically based on the provided parameters.     \n",
    "    def __init__(self, n_in, n_out, layers, optimizer = None, activation = 'relu', activation_last_layer = 'softmax',\n",
    "                 dropout_ratio = 0, is_batch_normalization = False, momentum = 0.9):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "\n",
    "        # initialize layers\n",
    "        self.layers=[]\n",
    "        self.activation= activation\n",
    "        self.optimizer = optimizer\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        for i in range(len(layers)): \n",
    "            if i == 0:\n",
    "                # adding the first hidden layer\n",
    "                # we fix activation to be one kind to compare performance\n",
    "                self.layers.append(LinearLayer(n_in,layers[i]))\n",
    "            else:\n",
    "                # adding the rest of hidden layers\n",
    "                self.layers.append(LinearLayer(layers[i-1],layers[i]))\n",
    "                \n",
    "            # we assume the MLP structure is Linear -- Dropout -- Batch Normlization\n",
    "            # dropout_ratio == 0 means no dropout\n",
    "            if dropout_ratio != 0:\n",
    "                self.layers.append(Dropout(dropout_ratio))\n",
    "                \n",
    "            if is_batch_normalization:\n",
    "                self.layers.append(Batch_Normalization())\n",
    "            \n",
    "            if i!=0:\n",
    "                self.layers.append(Activation(activation))\n",
    "        \n",
    "        # adding the output layer\n",
    "        self.layers.append(LinearLayer(layers[-1], n_out))\n",
    "        self.layers.append(Activation(activation_last_layer))\n",
    "\n",
    "    # forward progress: pass the information through the layers and out the results of final output layer\n",
    "    def forward(self,input_v):\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(input_v)\n",
    "            input_v = output\n",
    "        return output\n",
    "   \n",
    "    # backward progress  \n",
    "    def backward(self,delta):\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    # define the objection/loss function, we use cross_entropy as the loss\n",
    "    def criterion_cross_entropy(self,y,y_hat):\n",
    "        \"\"\"\n",
    "        y_hat: batch_size * 1\n",
    "        y : batch_size * n_class\n",
    "        y_actual_onehot: one hot encoding of y_hat, (batch_size * n_class)\n",
    "        \"\"\"\n",
    "        # cross entropy\n",
    "        y_actual_onehot = np.eye(self.n_out)[y].reshape(-1, self.n_out)\n",
    "        \n",
    "        # restrict the range into [1e-12, 1-1e-12]\n",
    "        y_predicted = np.clip(y_hat, 1e-12, 1-1e-12)\n",
    "        \n",
    "        # sum by row\n",
    "        loss = - np.sum(np.multiply(y_actual_onehot, np.log(y_predicted)), axis = 1)\n",
    "        # add weight decay parameter\n",
    "#         weight_decay_loss = loss * weight_decay\n",
    "        \n",
    "        # derivative of cross entropy with softmax\n",
    "        # self.layers[-1] is the activation function\n",
    "        delta = self.layers[-1].f_deriv(y_actual_onehot, y_predicted)\n",
    "        # return loss and delta\n",
    "        return loss, delta\n",
    "\n",
    "    # update the network weights after backward.\n",
    "    def update(self,lr, weight_decay):\n",
    "        if self.optimizer is None or self.optimizer == 'SGD':\n",
    "            optimizer_object = SGD(lr)\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__ =='LinearLayer':\n",
    "                    layer.update(optimizer_object, weight_decay)\n",
    "                elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "                    layer.update(optimizer_object, weight_decay)\n",
    "#                 if layer.__class__.__name__ =='LinearLayer':\n",
    "#                     layer.W = optimizer_object.update(layer.W, layer.grad_W) *  weight_decay\n",
    "#                     print(layer.W)\n",
    "#                     layer.b = optimizer_object.update(layer.b, layer.grad_b) *  weight_decay\n",
    "#                     print(layer.b)\n",
    "#                 elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "#                     layer.beta = optimizer_object.update(layer.beta, layer.dbeta) *  weight_decay\n",
    "#                     layer.gamma = optimizer_object.update(layer.gamma, layer.dgamma) *  weight_decay\n",
    "                    \n",
    "#         elif self.optimizer == 'Momentum':\n",
    "#             optimizer_object = Momentum(lr)\n",
    "#             for layer in self.layers:\n",
    "#                 if layer.__class__.__name__ =='LinearLayer':\n",
    "#                     layer.v_W, layer.W = optimizer_object.update(layer.v_W, layer.W, layer.grad_W)\n",
    "#                     layer.v_b, layer.b = optimizer_object.update(layer.v_b, layer.b, layer.grad_b)\n",
    "\n",
    "#                 elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "#                     layer.v_gamma, layer.gamma = optimizer_object.update(layer.v_gamma, layer.gamma, layer.dgamma)\n",
    "#                     layer.v_beta, layer.beta = optimizer_object.update(layer.v_beta, layer.beta, layer.dbeta)\n",
    "\n",
    "#         elif self.optimizer == 'NAG':\n",
    "#             optimizer_object = NAG(lr)\n",
    "#             for layer in self.layers:\n",
    "#                 if layer.__class__.__name__ =='LinearLayer':\n",
    "#                     layer.v_W, layer.W = optimizer_object.update(layer.v_W, layer.W, layer.grad_W)\n",
    "                    \n",
    "#                     layer.v_b, layer.b = optimizer_object.update(layer.v_b, layer.b, layer.grad_b)\n",
    "                    \n",
    "#                 elif layer.__class__.__name__ =='Batch_Normalization':\n",
    "#                     layer.v_gamma, layer.gamma = optimizer_object.update(layer.v_gamma, layer.gamma, layer.dgamma)\n",
    "#                     layer.v_beta, layer.beta = optimizer_object.update(layer.v_beta, layer.beta, layer.dbeta)\n",
    "                \n",
    "#         elif self.optimizer == 'Adagrad':\n",
    "#             optimizer_object = Adagrad(lr)\n",
    "#         elif self.optimizer == 'Adam':\n",
    "#             optimizer_object = Adam(lr)\n",
    "            \n",
    "#         for layer in self.layers:\n",
    "#             if layer.__class__.__name__ =='LinearLayer':\n",
    "#                 if self.optimizer is None or self.optimizer == 'SGD':\n",
    "#                     optimizer_object = SGD(lr)\n",
    "#                     layer.W = optimizer_object.update(layer.W, layer.grad_W)\n",
    "#                     layer.b = optimizer_object.update(layer.b, layer.grad_b)\n",
    "                    \n",
    "#                 elif self.optimizer == 'Momentum':\n",
    "#                     optimizer_object = Momentum(lr)\n",
    "#                     layer.v_W, layer.W = optimizer_object.update(layer.v_W, layer.grad_W, layer.W)\n",
    "#                     layer.v_b, layer.b = optimizer_object.update(layer.v_b, layer.grad_b, layer.b)\n",
    "                    \n",
    "    # define the training function\n",
    "    # it will return all losses within the whole training process.\n",
    "    def fit(self, X,y, is_shuffle = False, learning_rate=0.02, epochs=100, batch_size = 100, weight_decay = 1):\n",
    "        \"\"\"\n",
    "        Online learning with mini-batch training.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\" \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        # initialize loss array for epoch training \n",
    "        to_return = np.zeros(epochs)\n",
    "\n",
    "        n_batch = math.ceil(X.shape[0] / batch_size)\n",
    "                \n",
    "        for k in range(epochs):\n",
    "            time_start = time.time()\n",
    "            y_pred = None\n",
    "            \n",
    "            if is_shuffle == True:\n",
    "                X, y = shuffle(X, y)\n",
    "    \n",
    "            #initialize loss array for mini-batch training \n",
    "            batch_loss = np.zeros(n_batch)\n",
    "            \n",
    "            for j in range(n_batch):\n",
    "                \n",
    "                # decide samples for each mini-batch\n",
    "                if j != n_batch-1:\n",
    "                    X_batch = X[j*batch_size:(j+1)*batch_size]\n",
    "                    y_batch = y[j*batch_size:(j+1)*batch_size]\n",
    "                else:\n",
    "                    X_batch = X[j*batch_size:]\n",
    "                    y_batch = y[j*batch_size:]\n",
    "                    \n",
    "                # forward pass\n",
    "                y_batch_hat = self.forward(X_batch)\n",
    "\n",
    "                # calculate loss and backward pass\n",
    "                loss, delta = self.criterion_cross_entropy(y_batch, y_batch_hat)\n",
    "                self.backward(delta)\n",
    "                # update\n",
    "                self.update(learning_rate, weight_decay)\n",
    "                \n",
    "                # calculate loss and accuracy \n",
    "                batch_loss[j] = np.sum(loss)\n",
    "\n",
    "                #after argmax\n",
    "                y_batch_pred = np.argmax(y_batch_hat, axis = 1).reshape(-1,1)\n",
    "#                 print(y_batch_hat)\n",
    "#                 print(y_batch_pred) \n",
    "                if y_pred is None:\n",
    "                    y_pred = y_batch_pred\n",
    "                else:\n",
    "                    y_pred = np.vstack((y_pred, y_batch_pred))\n",
    "\n",
    "            epoch_loss = np.mean(batch_loss)\n",
    "            \n",
    "            to_return[k] = epoch_loss\n",
    "            \n",
    "            # for every epoch, print time and loss\n",
    "            accuracy = np.sum(y_pred == y) / y.shape[0]\n",
    "            print(\"epoch {} loss {:.6f}, accuracy {:.6f}% \".format(k, epoch_loss, 100*accuracy))\n",
    "        return to_return\n",
    "    \n",
    "    # define the prediction function\n",
    "    # we can use predict function to predict the results of new data, by using the well-trained network.\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-tattoo",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "historic-clearance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,10) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-172b13c07217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m nn = MLP(n_input_training, n_class, layers, optimizer, activation = 'relu', activation_last_layer = 'softmax',\n\u001b[1;32m      6\u001b[0m                  dropout_ratio = 0, is_batch_normalization = False)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-229-9b97a184afd0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, is_shuffle, learning_rate, epochs, batch_size, weight_decay)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0my_batch_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# calculate loss and backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-9b97a184afd0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_v)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0minput_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-27e755197eba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \"\"\"\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-27e755197eba>\u001b[0m in \u001b[0;36m__softmax\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         res = x_exp / x_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0me\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,10) (100,) "
     ]
    }
   ],
   "source": [
    "n_input_training = x_train_norm.shape[1]\n",
    "n_class = len(np.unique(test_label.reshape(1, -1)))\n",
    "layers = [26, 12]\n",
    "optimizer = 'SGD'\n",
    "nn = MLP(n_input_training, n_class, layers, optimizer, activation = 'relu', activation_last_layer = 'softmax',\n",
    "                 dropout_ratio = 0, is_batch_normalization = False)\n",
    "nn.fit(x_train_norm, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-petite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-craft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
